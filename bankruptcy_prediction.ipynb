{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeTsTRh-rjb-"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UnrdCUC2rDZ9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_selector\n",
        "from imblearn.pipeline import make_pipeline\n",
        "\n",
        "from scipy.stats import alpha, randint, uniform\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
        "\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "from google.colab import drive\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNYbOI6qENdh"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project uses a dataset of financial data for Polish companies analyzed in the period of 2000-2012. The full dataset can be found on: https://archive.ics.uci.edu/dataset/365/polish+companies+bankruptcy+data\n",
        "\n",
        "*Tomczak,Sebastian. (2016). Polish companies bankruptcy data. UCI Machine Learning Repository.*\n",
        "\n",
        "In this project we are going to use a subset of only the data collected in the first year of the forecasting period.Specifically, the dataset consist of 64 features for each instance (X1-x64) representing financial data for each company and a class label (X65) that indicates bankrupty status after 5 years (0 for negative and 1 for positive). To check the financial measurments that each future reprents you can see the [Dataset Features](#scrollTo=j1E6vpILbQjv) section.\n",
        "\n",
        "The goal of the project is to develop a model that will predict bankrupty status in 5 years based on the financial statistics that are provided in this dataset. The main challenge of the project is that the dataset has a lot of inconsistencies. For example the dataset has missing values, outliers, duplicate entries, small amount of instancies, unbalanced class distribution.\n",
        "We will see the problems of the dataset below and we will discuss the things we need to fix before we pass the data in ML algorithms. Then we will show the preprocessing tools that we can use to fix the problems and see how they work. Finally we are going to train and evaluate different Machine Learning algorithms and see how they peform on the test set."
      ],
      "metadata": {
        "id": "pg3hbGOV2vjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "fO1AQheH26zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "pZmn1tmmIlOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the training and test sets"
      ],
      "metadata": {
        "id": "E8ZemH5CIrTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.read_csv('https://raw.githubusercontent.com/NikitasThermos/bankruptcy_prediction/main/Dataset/companydata.csv')\n",
        "X_test = pd.read_csv('https://raw.githubusercontent.com/NikitasThermos/bankruptcy_prediction/main/Dataset/test_data.csv')\n",
        "y_test = pd.read_csv('https://raw.githubusercontent.com/NikitasThermos/bankruptcy_prediction/main/Dataset/test_labels.csv', header=None)"
      ],
      "metadata": {
        "id": "RnVC39MLUZBZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Features"
      ],
      "metadata": {
        "id": "j1E6vpILbQjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X1\tnet profit / total assets\n",
        "\n",
        "X2\ttotal liabilities / total assets\n",
        "\n",
        "X3\tworking capital / total assets\n",
        "\n",
        "X4\tcurrent assets / short-term liabilities\n",
        "\n",
        "X5\t[(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] * 365\n",
        "\n",
        "X6\tretained earnings / total assets\n",
        "\n",
        "X7\tEBIT / total assets\n",
        "\n",
        "X8\tbook value of equity / total liabilities\n",
        "\n",
        "X9\tsales / total assets\n",
        "\n",
        "X10\tequity / total assets\n",
        "\n",
        "X11\t(gross profit + extraordinary items + financial expenses) / total assets\n",
        "\n",
        "X12\tgross profit / short-term liabilities\n",
        "\n",
        "X13\t(gross profit + depreciation) / sales\n",
        "\n",
        "X14\t(gross profit + interest) / total assets\n",
        "\n",
        "X15\t(total liabilities * 365) / (gross profit + depreciation)\n",
        "\n",
        "X16\t(gross profit + depreciation) / total liabilities\n",
        "\n",
        "X17\ttotal assets / total liabilities\n",
        "\n",
        "X18\tgross profit / total assets\n",
        "\n",
        "X19\tgross profit / sales\n",
        "\n",
        "X20\t(inventory * 365) / sales\n",
        "\n",
        "X21\tsales (n) / sales (n-1)\n",
        "\n",
        "X22\tprofit on operating activities / total assets\n",
        "\n",
        "X23\tnet profit / sales\n",
        "\n",
        "X24\tgross profit (in 3 years) / total assets\n",
        "\n",
        "X25\t(equity - share capital) / total assets\n",
        "\n",
        "X26\t(net profit + depreciation) / total liabilities\n",
        "\n",
        "X27\tprofit on operating activities / financial expenses\n",
        "\n",
        "X28\tworking capital / fixed assets\n",
        "\n",
        "X29\tlogarithm of total assets\n",
        "\n",
        "X30\t(total liabilities - cash) / sales\n",
        "\n",
        "X31\t(gross profit + interest) / sales\n",
        "\n",
        "X32\t(current liabilities * 365) / cost of products sold\n",
        "\n",
        "X33\toperating expenses / short-term liabilities\n",
        "\n",
        "X34\toperating expenses / total liabilities\n",
        "\n",
        "X35\tprofit on sales / total assets\n",
        "\n",
        "X36\ttotal sales / total assets\n",
        "\n",
        "X37\t(current assets - inventories) / long-term liabilities\n",
        "\n",
        "X38\tconstant capital / total assets\n",
        "\n",
        "X39\tprofit on sales / sales\n",
        "\n",
        "X40\t(current assets - inventory - receivables) / short-term liabilities\n",
        "\n",
        "X41\ttotal liabilities / ((profit on operating activities + depreciation) * (12/\n",
        "365))\n",
        "\n",
        "X42\tprofit on operating activities / sales\n",
        "\n",
        "X43\trotation receivables + inventory turnover in days\n",
        "\n",
        "X44\t(receivables * 365) / sales\n",
        "\n",
        "X45\tnet profit / inventory\n",
        "\n",
        "X46\t(current assets - inventory) / short-term liabilities\n",
        "\n",
        "X47\t(inventory * 365) / cost of products sold\n",
        "\n",
        "X48\tEBITDA (profit on operating activities - depreciation) / total assets\n",
        "\n",
        "X49\tEBITDA (profit on operating activities - depreciation) / sales\n",
        "\n",
        "X50\tcurrent assets / total liabilities\n",
        "\n",
        "X51\tshort-term liabilities / total assets\n",
        "\n",
        "X52\t(short-term liabilities * 365) / cost of products sold)\n",
        "\n",
        "X53\tequity / fixed assets\n",
        "\n",
        "X54\tconstant capital / fixed assets\n",
        "\n",
        "X55\tworking capital\n",
        "\n",
        "X56\t(sales - cost of products sold) / sales\n",
        "\n",
        "X57\t(current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)\n",
        "\n",
        "X58\ttotal costs /total sales\n",
        "\n",
        "X59\tlong-term liabilities / equity\n",
        "\n",
        "X60\tsales / inventory\n",
        "\n",
        "X61\tsales / receivables\n",
        "\n",
        "X62\t(short-term liabilities *365) / sales\n",
        "\n",
        "X63\tsales / short-term liabilities\n",
        "\n",
        "X64\tsales / fixed assets\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vj-5QLLebWfc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTLGU3-THZTj"
      },
      "source": [
        "## Exploring the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the first five instances of the dataset below:  "
      ],
      "metadata": {
        "id": "lJkfHlKGuLsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyDxiLk2vyyH"
      },
      "outputs": [],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all of the features consist of numerical data:"
      ],
      "metadata": {
        "id": "ep_4ovRjZz_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GfZz47g5lpc"
      },
      "outputs": [],
      "source": [
        "training_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see many features have different number of non-null values so we suspect that there are missing values in the dataset.We can check the perchentage of missing values for each column below:  "
      ],
      "metadata": {
        "id": "zw40OWb1gmUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percentage = training_df.isnull().mean() * 100\n",
        "for col, per in zip(missing_percentage.index, missing_percentage):\n",
        "  print(f'{col}, {per:.2f}%')"
      ],
      "metadata": {
        "id": "fBB2eiMmgt9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because most of the Machine Learning algorithms can't work with missing values we have to address this in the preprocessing of the dataset."
      ],
      "metadata": {
        "id": "FirNjuXqp_Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features have a variety of ranges and there some outlier values as we see extreme mins-maxs compared to the mean:"
      ],
      "metadata": {
        "id": "b9wGHsWbtbm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.describe()"
      ],
      "metadata": {
        "id": "QMDsGIDetdsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most Machine Learning algorithms we will have to bring all the features to the same range to help the training. This is usually done with a Scaler."
      ],
      "metadata": {
        "id": "M2bdJzsqtilJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this subset we have data for 7027 companies from which the 6756(96%) belong to the negative class and 271(4%) to the positive class"
      ],
      "metadata": {
        "id": "0Wxe822Ua6s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of companies: {training_df.shape[0]}')\n",
        "print(f'Number of negative class: {np.sum(training_df[\"X65\"]==0)}')\n",
        "print(f'Number of positive class: {np.sum(training_df[\"X65\"]==1)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_cp76EXrat3",
        "outputId": "718ceb77-3a7e-4cb9-facd-25e2cde8e0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of companies: 7027\n",
            "Number of negative class: 6756\n",
            "Number of positive class: 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unbalance between the negative and positive classes may cause problems in the training of some ML algorithms. In some extreme cases the models will learn to qualify every instance as the majority class because this will give a good enough accuracy."
      ],
      "metadata": {
        "id": "ptVKXCf5MGLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlations"
      ],
      "metadata": {
        "id": "R3xQf-pyvKoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the measurments that we can use to check the importance of each feature is the Standard Corrlation Coefficient (Pearson's r).\n",
        "This can measure the relationship between every pair of attributes but it measures only the linear correlations. The results ranges from -1 to 1. When it is close to 1 there is a strong positive correlation (when one attribute increases then the other one increases linearly). When it is close to -1 there is a negative correlation (when one attribute increases the other one decreases linearly). Coefficients close to 0 mean that there is no linear correlation.\n",
        "\n",
        "For now we can check the correlations of all features with the label column. Because the labels are binary (0 or 1) we may not get as usefull results as if we had a regression task."
      ],
      "metadata": {
        "id": "-pBTwqPsw8B0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8yybcDexYHk"
      },
      "outputs": [],
      "source": [
        "corr_matrix = training_df.corr()\n",
        "\n",
        "corr_matrix['X65'].sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may also check the correlation matrix for all features and remove one feature of the pairs that have strong correlation. Removing one feature from the correlated pairs will not remove much information because the remaining feature can represent both and at the same time we simplify the dataset. At this case, because we do not have that many features, we decide that we do not need to remove any features."
      ],
      "metadata": {
        "id": "HayGg-9p4l2U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ZwB3XWGzcK"
      },
      "source": [
        "## Histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms are a nice way to quickly check the distribution of each feature and find out some \tpeculiarities that they may have, such as being soft or hard capped and following power law distribution\n",
        "At this momment the histograms will not give as much informations because of the outlier values that exist in the dataset.\n",
        "\n",
        "We splited the histograms in multiple cells for easier observation of each figure   "
      ],
      "metadata": {
        "id": "BqLsWp1jnT8S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYsjJmRnGy6W"
      },
      "outputs": [],
      "source": [
        "training_df.iloc[:, :10].hist(bins=10, figsize=(20, 15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7trUHpAuHm3Z"
      },
      "outputs": [],
      "source": [
        "training_df.iloc[:, 10:20].hist(bins=10, figsize=(20, 15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPOu5bzLIJFW"
      },
      "outputs": [],
      "source": [
        "training_df.iloc[:, 20:30].hist(bins=10, figsize=(20, 15))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.iloc[:, 30:40].hist(bins=10, figsize=(20, 15))"
      ],
      "metadata": {
        "id": "nH51if5-RCe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.iloc[:, 40:50].hist(bins=10, figsize=(20, 15))"
      ],
      "metadata": {
        "id": "D14xjbxyRFFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.iloc[:, 50:60].hist(bins=10, figsize=(20, 15))"
      ],
      "metadata": {
        "id": "Oap7WV8fRHZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.iloc[:, 60:65].hist(bins=10, figsize=(20, 15))"
      ],
      "metadata": {
        "id": "N56hvB7yRJJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "lfCW0WxF4E_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While examinig the dataset we noticed that there are duplicate instances. Because it is higly unlikely that there are instances that are sharing the exactly same values across all the features, we suspect that they were mistakenly added in the set.\n",
        "\n",
        "Also, all the duplicate instances are part of the negative class which is by far larger, so we think that keeping those duplicates will not provide more information and they will shadow even more the positive class.\n",
        "\n",
        "For those reasons we decide to remove the duplicate instaces (82 total)"
      ],
      "metadata": {
        "id": "ya-BIteWVpjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = training_df.drop_duplicates()"
      ],
      "metadata": {
        "id": "gsd-LLJY4Lw9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this momment we can also spit the training features(X1-X64) and the class labels (X65):"
      ],
      "metadata": {
        "id": "UtpI5woTWaYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = new_df.drop('X65', axis = 1), new_df['X65']"
      ],
      "metadata": {
        "id": "Yw4aTgTC4N-J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7IG-760FETH"
      },
      "source": [
        "## Remove extreme values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen that there are some extreme values by comparing the mean of each feature to the min-max.\n",
        "\n",
        "The extreme values can disrupt the training of the models as well as the preprocessing steps. For example many scalers will squash most of the values to a very small range due to the outlier values.\n",
        "\n",
        "For these reasons we remove the outlier values and replace them with NaN. Later we are going to see how to fill the missing values."
      ],
      "metadata": {
        "id": "zIUUyyntc-IY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U7XyCN0RuV0B"
      },
      "outputs": [],
      "source": [
        "threshold = 2\n",
        "\n",
        "scaler = StandardScaler()\n",
        "z_scores = scaler.fit_transform(X_train)\n",
        "\n",
        "outliers_mask = np.abs(z_scores) > threshold\n",
        "X_train[outliers_mask] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LWJGEQ2zoUvs"
      },
      "outputs": [],
      "source": [
        "threshold = 2\n",
        "scaler = StandardScaler()\n",
        "z_scores = scaler.fit_transform(X_test)\n",
        "\n",
        "outliers_mask = np.abs(z_scores) > threshold\n",
        "X_test[outliers_mask] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "5uCv1sfpP4-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A preprocessing pipeline is needed to apply all the transformations we want on the dataset. A pipeline collects all the preprocessing steps into one structure and applies them sequentially. We are not going to have a single global preprocessing pipeline that we will create here, but instead we are going to create a pipeline each time we train a different model. There a few reasons for that:\n",
        "\n",
        "\n",
        "1. At the end of each preprocessing pipeline we can add the model that we are training each time. With that we can produce a pipeline that applies all the preprocessing steps and feeds the data to the model automatically. Thus we can have a pipeline that automatically receives new data, transforms them to the desired format and makes predictions at the end.\n",
        "2. When we train each model we want to run a grid search that searches for the optimal hyperparameters for better perfomance. A lot of preprocessing components also include hyperparameters that can impact the perfomance of the model. By combining the preprocessing components and the classifier model together we can run a grid search at the same time for all of them.    \n",
        "3. We may not want to apply the same preprocessing steps for each differnt ML model\n",
        "\n",
        "At this section we discuss what preprocessing steps we are going to use and how they work.\n",
        "\n",
        "1) **Imputing**\n",
        "\n",
        "One of the most important steps when processing data for ML algorithms is the imputing. Imputing will fill the missing values of the dataset. This is not only a perfomance issue because most of the of the models will not work at all when they come across missing values. Many times also we may need to include that step in our model even if the training data have not missing values if we cannot guarantee that future instances will also have not missing values.\n",
        "\n",
        "One simply way to do this is to calculate some statistics about each feature such as the mean or the median and replace the missing values with them. This can be done easily with SimpleImputer. Instead we use the KNNImputer which uses the K-nearest Neighbors algorihtm to compute the closest neighbors for each instance (the instances that are more similar for each instance) and then replace the missing values with the mean value of the n-nearest neighbors.\n",
        "\n",
        "2) **Scaling**\n",
        "\n",
        "Scaling of the data is crucial for the perfomance of the most ML models especially when the features of the data have different distributions and ranges. Scaling the data can lead to more stable training, especially in the cases of iterative optimization algorithms, such as gradient descent.\n",
        "\n",
        "The most typical way to scale the data is standarization which is done by removing the mean and dividing by the standard deviation.\n",
        "This can be done with StandardScaler but the Scikit-Learn's documentation notes that this scaler is sensitive to outliers and cannot guarantee balanced feature scales. Therefore, we are going to use the RobustScaler, which is based on the percentiles of the data and therefore not influenced by outliers.\n",
        "\n",
        "\n",
        "3) **Polynomial Features**\n",
        "\n",
        "Due to a small amount of features our dataset has we can generate more by adding a higher degree of each feature and combinations of them. This can be done with PolynomialFeatures. For example, if we have two features a and b and add features up to degree=3 then the new features will be: $$a^2,\n",
        "a^3 ,b^2, b^3, a^2b, ab^2$$\n",
        "\n",
        "If we select degree=d and the dataset has n features then number of the features the new dataset will have can be given by:\n",
        "$$ \\frac{(n + d)!}{d!n!} $$\n",
        "\n",
        "4) **Sampling**\n",
        "\n",
        "We noticed that there is a huge imbalance between the positive and negative classes. That can lead to problems in the training procedure as the models will rarely observe postive instances, thus pushing the parameters of the model to classify more instances as negative. In some cases the models learn to classify all the isntances as the majority class completely ignoring the minority class.\n",
        "\n",
        "In this cases one thing we can do is set the class weight hyperparameter to 'balanced' for our models. This can ensure that the models will focus more on the minority class instances\n",
        "\n",
        "One more technique that we can use in those situatuions is the over-sampling of the minority class, which wiil generate new instances of the minority class so the two classes will be more balanced. Two famous algortihms are the Synthetic Minortiy Oversampling Technique (SMOTE) and the Adaptive Synthetic (ADASYN). Both use the same algorithm to generate new samples which is\n",
        "\n",
        "$$ x_{new} = x_i + λ(x_{zi} - x_i)$$\n",
        "\n",
        "This interpolation will generate a sample between $ x_i$ and $x_{zi}$ with λ being a random number in the range [0, 1].\n",
        "The main difference of the methods is on the selection process of the instances that are going to be used for the generation of the new instances.\n"
      ],
      "metadata": {
        "id": "Vl2bCdf2P6nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perfomance Measures"
      ],
      "metadata": {
        "id": "cyoPXjB3ANj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen before our dataset is heavily imbalanced. For that reason, having accuracy as a perfomance measure is not optimal. If the model qualifies every instance in the training set as negative the accuracy of the model will be 96% (because 96% of the instances belong to the negative class) which seems as great perfomance but in reality the model would have not learned anything.\n",
        "Instead we can measure the accuracy that model shows in each class. Since this a binary classification task we are going to count the following:\n",
        "\n",
        "\n",
        "*   True Negatives (TN) : Negative instances that were correctly claassified as negative\n",
        "*   False Negatives (FN) : Positive instances that were incorrectly classified as negative\n",
        "*   True Positives (TP) : Positive instances that were correctly classified as positive\n",
        "*   False Possitives (FP) : Negative instances that were incorrectly classified as positive\n",
        "\n",
        "To collect all of this together we can use a confusion matrix which displays the above measurments in the following format:\n",
        "\n",
        " $$\n",
        " \\begin{bmatrix}\n",
        " TN & FP \\\\\n",
        " FN & TP \\\\\n",
        " \\end{bmatrix}\n",
        " $$\n",
        "\n",
        "Obviously an ideal model would have a confusion matrix with instances only in the top left and bottom right corners.\n",
        "\n",
        "From these also arise two more measures, Precision and Recall. Precision measures how many of the instances classified as positive are actually positive. In other words, when the model classifies an instance as positive how many time it is correct.\n",
        "\n",
        "$$ Precision = \\frac{TP}{TP + FP} $$\n",
        "\n",
        "Recall measures how many of the positive instances are classified as positive. In other words, how good is the model at finding the positive instances.\n",
        "\n",
        "$$ Recall = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Many times there is a trade-off between the two, meaning that when one increases the other one decreases. For that reason, we can also combine these two to get the F1 score. The F1 score is the harmonic mean between Precision and Recall.\n",
        "\n",
        "$$F1 = \\frac{2}{\\frac{1}{Precision}+\\frac{1}{Recall}} = 2\\frac{precision × recall}{precision + recall} = \\frac{TP}{TP + \\frac{FN + FP}{2}}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fcs5sjV_wwj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "def log_results(model, y_true, y_pred):\n",
        "  conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  print(f'confusion matrix for {model}:\\n {conf_matrix}')\n",
        "  print(f'Precision Score: {precision:.2f}')\n",
        "  print(f'Recall Score: {recall:.2f}')\n",
        "  print(f'F1 Score: {f1:.2f}')\n",
        "  results.append([model, precision, recall, f1])"
      ],
      "metadata": {
        "id": "hpaNXi9AASPs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxsgdfsDJ8Qb"
      },
      "source": [
        "# SGDClassifier - LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "9WQP_bj8Tk7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This estimator uses Stochastic Gradient Descent to fit different models based on the loss hyperparameter. By default it fits a linear Support Vector Machine. It also adds a regularization term Which can be L1, L2 or Elasticnet which is a combination of both.\n",
        "\n",
        "\n",
        "Gradient Descent is an iterative optimization algorithm capable of finding the minimum of multivariate function.\n",
        "\n",
        "In binary classification the model needs to output a probability that an instance belongs to a class. For that reason the output of the model passes through a Logistic function:\n",
        "$$ σ(t) = \\frac{1}{1 + e^{-t}}$$\n",
        "\n",
        "So the output of the model is:\n",
        "$$h_θ(x) = σ(θ^Τx)$$\n",
        "\n",
        "With θ being the parameter vector of the model and x being the input. The model predicts a probability $\\hat{p}$ that an instance belongs to the postive class and $1-\\hat{p}$ that it belongs to the negative class. For that reason the cost function for a single instance can be:\n",
        "$$c(θ) = \\cases{-log(h_θ(x)) \\ \\ \\ \\ \\ \\ \\ \\ \\text{if y=1} \\\\-log(1-h_θ(x)) \\ \\text{if y=0}}$$\n",
        "\n",
        "The reason that -log(p) is close to 0 when p approaches 1 and lagre when it approaches 0. So if positive instances classified as negative there is going to be a large loss and vice versa. If we combine the above for all instance we get the log loss:\n",
        "$$J(θ) = -\\frac{1}{m}\\sum^m_{i=1}[y_ilog(h_θ(x_i)) + (1-y_i)log(1-h_θ(x_i))]$$\n",
        "\n",
        "Now we need to minimize the cost function so the model can work optimally. In iterative methods we do that by looking at which point of the cost function we are at each time and modifying the parameters of the model in a way that leads us closer to the global minimum. To find the way that leads us to the global minimum we need to calculate the partial derivatives of the cost function with regard to every parameter of the model. The partial derivative of the log loss with regard to the jth parameter can be given by:\n",
        "\n",
        "$$\\frac{∂}{\\partialθ_j}[y_ilog(h_θ(x_i)) + (1-y_i)log(1-h_θ(x_i))] =$$\n",
        "\n",
        "$$y_i \\frac{1}{h_θ(x_i)}\\frac{∂h_θ(x_i)}{∂θ_j} + (1 - y_i)\\frac{1}{1-h_θ(x_i)}(-\\frac{∂h_θ(x_i)}{∂θ_j}) = $$\n",
        "\n",
        "$$(\\frac{y_i}{h_θ(x_i)} - \\frac{1-y_i}{1 - h_θ(x_i)})\\frac{∂h_θ(x_i)}{∂θ_j}=$$\n",
        "\n",
        "$$(\\frac{y_i - y_ih_θ(x_i)-h_θ(x_i)+y_ih_θ(x_i)}{h_θ(x_i)(1-h_θ(x_i))})\\frac{∂h_θ(x_i)}{∂θ_j} =$$\n",
        "\n",
        "$$(\\frac{y_i-h_θ(x_i)}{h_θ(x_i)(1-h_θ(x_i))})\\frac{∂h_θ(x_i)}{∂θ_j} (1)$$\n",
        "\n",
        "\n",
        "Remember that, $h_θ(x) = σ(θ^Τx)$ and the derivative of that with regard to $θ_j$ is:\n",
        "$$\\frac{∂σ(θ^Τx)}{∂θ_j} = σ(θ^Τx)(1-σ(θ^Τx))\\frac{∂}{∂θ_j}θx_j = h_θ(x)(1-h_θ(x))x_j$$\n",
        "\n",
        "And if we replace that in equation (1):\n",
        "\n",
        "$$(\\frac{y_i-h_θ(x_i)}{h_θ(x_i)(1-h_θ(x_i))})h_θ(x_i)(1-h_θ(x_i))x_{ij} = $$\n",
        "\n",
        "$$(y_i - h_θ(x_i))x_{ij} $$\n",
        "\n",
        "So if we introduce again the sum:\n",
        "$$\\frac{∂}{∂θ_j}J(θ) = -\\frac{1}{m}∑_{i=1}^m( y_i - h_θ(x_i))x_{ij}$$\n",
        "\n",
        "$$\\frac{∂}{∂θ_j}J(θ) = \\frac{1}{m}\\sum^m_{i=1}(σ(θ^Τx_i) - y_i)x_{ij}$$\n",
        "\n",
        "Now we need to calculate that for every parameter of the model, so we need to get the gradient vector fo the cost function:\n",
        "$$∇_θJ(θ) =  \n",
        " \\begin{pmatrix}\n",
        " \\frac{∂}{∂θ_0}J(θ) \\\\\n",
        " \\frac{∂}{∂θ_1}J(θ) \\\\\n",
        " ... \\\\\n",
        " \\frac{∂}{∂θ_n}J(θ) \\\\\n",
        " \\end{pmatrix} =\n",
        " \\frac{1}{m} X^T(h_θ(Χ) -y)\n",
        " $$\n",
        "\n",
        "Note that gradient vector's components are all the partial derivatives of the function. With that we can find which way we need to update our parameters to get closer to the global minimum. Because we need to descent, we subtract the gradient vector from the current parameter values to obtain the new parameters.\n",
        "\n",
        "$$ θ^{next} = θ -η∇_θJ(θ)$$\n",
        "\n",
        "With η being the learning rate which controls the size of steps we take at each iteration to stabilize the descent towards the minimum.\n",
        "\n",
        "We iterate this process until we find the minimum (the gradient in minimum is zero). This is the process that the model follows only for the log loss but as we said SGDClassifier can also use other methods and use Gradient Descent to train them.\n",
        "\n",
        "As we mentioned above we can also add regularization terms to control the parameters of the model.\n",
        "\n",
        "1) Ridge Regression\n",
        "\n",
        "Ridge regression adds a term to the cost function that corresponds to the l2 norm of the weight vector:\n",
        "\n",
        "$$ J(θ) + \\frac{a}{m}∑_{i=1}^nθ_i^2$$\n",
        "\n",
        "Now the model not only tries to minimize the cost function but also keep the parameters of the model as low as possible (Note that i starts from 1 because the bias parameter is not regularized).\n",
        "\n",
        "2) Lasso Regression\n",
        "\n",
        "Lasso regression adds a term that corresponds to the l1 norm of the weights vector:\n",
        "$$ J(θ) + 2α\\sum_{i=1}^n |θ_i| $$\n",
        "\n",
        "\n",
        "3) Elastic Net Regression\n",
        "\n",
        "Elastic Net is a combination of the previous regressions. We can control the ration between the two with hyperparameter r:\n",
        "$$ J(θ) + r(2α\\sum_{i=1}^n |θ_i|) + (1-r)( \\frac{a}{m}∑_{i=1}^nθ_i^2)$$"
      ],
      "metadata": {
        "id": "gpq3lRffTmzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "1SL9UitoTie8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_pipeline = make_pipeline(KNNImputer(weights='distance'), PolynomialFeatures(degree=2, include_bias=False),\n",
        "                              RobustScaler(), ADASYN(sampling_strategy='minority', random_state=42),\n",
        "                              SGDClassifier(loss='log_loss', learning_rate='adaptive', penalty='elasticnet', class_weight=None,\n",
        "                                            random_state=42))\n",
        "\n",
        "param_distribs = {'knnimputer__n_neighbors': randint(low=10, high=500),\n",
        "                  'adasyn__n_neighbors': randint(low=3, high=100),\n",
        "                  'sgdclassifier__alpha': uniform(loc=0.0001, scale=3),\n",
        "                  'sgdclassifier__l1_ratio': uniform(loc=0.1, scale=0.9),\n",
        "                  'sgdclassifier__eta0': uniform(loc=0.0001, scale=10),\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    full_pipeline,\n",
        "    param_distributions=param_distribs,\n",
        "    n_iter=10,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=4,\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "random_search.best_params_\n",
        "\n"
      ],
      "metadata": {
        "id": "G9pNuo9kzObT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5055e794-f41f-4613-a24a-e415f571e537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adasyn__n_neighbors': 53,\n",
              " 'knnimputer__n_neighbors': 144,\n",
              " 'sgdclassifier__alpha': 0.5116723710618746,\n",
              " 'sgdclassifier__eta0': 0.6506159298527951,\n",
              " 'sgdclassifier__l1_ratio': 0.9539969835279999}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = random_search.cv_results_"
      ],
      "metadata": {
        "id": "xo_mUUUrJtuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_val_scores = random_search.cv_results_['mean_test_score']\n",
        "max(mean_val_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0bSIzM8VqE0",
        "outputId": "783a0857-1b92-404b-d3f1-874bf0f51e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19262574376522565"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDIz1ovCbE6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9202b79-0bca-481f-af38-fd0d2be8083e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix for LogLoss:\n",
            " [[2727  759]\n",
            " [ 212  302]]\n",
            "Precision Score: 0.28\n",
            "Recall Score: 0.59\n",
            "F1 Score: 0.38\n"
          ]
        }
      ],
      "source": [
        "y_pred = random_search.predict(X_test)\n",
        "log_results('LogLoss', y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(random_search, 'logloss.pki')"
      ],
      "metadata": {
        "id": "_BPGGueDCIsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD Classifier - SVM"
      ],
      "metadata": {
        "id": "ZcJIz_Lc1ZFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "1xdmVHG00DL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main goal of Support Vector Machines is not only to split the training set but also to keep the decision boundary as far away as possible from all the instances. In other words, in a binary classifcatoin SVMs try to fit the widest 'street' between the two classes.\n",
        "\n",
        "For each instance the SVMs calculate the decision function which is simple: $$s(x) = w^Tx + b $$\n",
        "With $w$ being the parameters of the mode, $x$ is the input instance and $b$ is the bias parameter. The model predicts the positive class if the the decision function is positive, otherwise it predicts the negative class. Thus, the decision boundary is at $s=0$.\n",
        "\n",
        "The model also keeps track of the margin between the decision boundary and the closest instances. On the edges of the margin, which are at $s ± 1$, lie the support vectors, the instances that are the closest to the decision boundary (Only in hard margin classification).Support vectors are very important, because when new instances added that are already outside of the margin defined by the support vectors do not have an impact on the model.\n",
        "\n",
        "This is hard margin classification. The model defines a margin around the decision boundary that not a single instance should be inside it. The logic behind it, is that by placing the decision boundary on the area of the largest seperation between the classes, we do not overfit the data and the model can generalize better.\n",
        "\n",
        "The model has two goals in the training procedure. First is to make the margin as large as possible. To enlarge the margin we need to decrease the weight vector. When decreasing the weight vector it means that when calculating the decision function we scale down even more the input instances. That will result in the instaces being closer at the edges of the margin. For example, if the input is $x_i=2$ and the $w=1$ then $s_i=2$ (skipping the bias for now), which will make the instance to have some distance from the edge. But if we decrease the $w$ to 0.5 then $s_i=1$ placing the instance right on edge of the margin, making the margin appear larger as it is now closer to the instances.\n",
        "\n",
        "The second goal is to keep all the instances off the margin, so all the positive instaces to be greater than 1 and all the negative instances lower than -1. That can be achived by adding a constraint as below:\n",
        "$$ t_i(w^Tx_i + b) ≥ 1 $$\n",
        "\n",
        "with $t_i = -1$ for negative instances $(y_i = 0)$ and $t_i = 1$ for positive instances $(y_i = 1)$\n",
        "\n",
        "So the objective for hard margin classification is given by:\n",
        "$$minimize_{w} \\frac{1}{2}w^Tw$$\n",
        "$$\\text{subject to } t_i(w^Tx_i + b) ≥ 1$$\n",
        "\n",
        "We minimize $\\frac{1}{2}w^Tw$ which is equivalent to $\\frac{1}{2}||w||^2$ because it has a better derivative than ||w||\n",
        "\n",
        "\n",
        "Sometimes hard margin classification is not the best choice. For example in a binary classification task, if we have just one outlier that lies in the general space of the opposite class, it would be impossible to find a hard margin boundary that seperates the two classes because that needs for all the instances to be on the right side of the boundary and none of them to be inside the margin. Problems arise even in the cases that the outlier is not mixed with the opposite class but it is very close to it. In that case there could be possible to find a hard margin boundary but it would be to close to the instances of that class and probably would not generalize well. For those reasons we can apply soft margin classification where we lift the constraint that all instances must be off the margin, but still trying to reduce the margin violations as much as possible. For that we can introduce a variable $ζ$ for each instance that specifies how much each instance should violate the margin. So now the objective becomes:\n",
        "$$minimize_{w} \\frac{1}{2}w^Tw + C∑_{i=1}^m ζ_i$$\n",
        "$$\\text{subject to } t_i(w^Tx_i + b) ≥ 1 - ζ_i \\ \\text{and} \\ ζ_i \\ge 0$$\n",
        "\n",
        "Now we have a trade-off, trying to make the margin as big as possible and at the same time limit the margin violations. This is why the C hyperparameter is introduced to let us specify which of those we want to focus more.\n",
        "\n",
        "\n",
        "Both hard margin and soft margin classification problems are known as quadratic programming (QP) problems. There are some off-the-shelf QP solvers that somenone can use to solve these kinds of problems, but one of the most effiecnt ways is to use gradient descent. In that context, we need to define a loss function which most of the times it is either the hinge or squared hinge loss. For the positive instances (t=1) the loss is zero if the decision function is greater than 1. Note that we still predict the positive class if the decision function is positive but there is still some loss for that instances (instances that are in the right side of the boundary but inside the margin). For the negative instances (t=-1) the loss is zero if the decision function is than -1. The loss grows larger as further away the instances are from the right side. The hinge loss can be given by:\n",
        "$$ J(x) = max(0, 1-ts(x))$$\n",
        "\n",
        "When the predictions are correct and off the margin we are going to have $ts(x) > 1$ (remember t is either -1 or 1 and s is less than -1 for correct negative predictions or greater than 1 for correct positive predictions), so $0 > 1 - ts(x)$ and the loss is going to be zero. If the instace is inside the margin but on the right side, $s ϵ(-1, 0) $ for negative instances or $sϵ(0, 1)$ for positive instances, we are going to have $0<ts(x)<1 => 0 < 1-ts(x) < 1 $ so the loss is going to be between 0 and 1. Finally, if the prediction is wrong t and s are going to have opposite signs so $ts(x) < -1 => 1-ts(x)>0$ so the loss is going to be as big as the s.\n",
        "\n",
        "To use the hinge loss for gradient descent we also need to calculate the partial derivative with resepct to every parameter of the model. That can be given by:\n",
        "$$\\frac{∂J}{∂w_i} = \\frac{∂}{∂w_i}1-ts(x)=\\frac{∂}{∂w_i}1-t(w_i^Tx_i+b) = -tx_i $$\n",
        "\n",
        "$$\\frac{J}{∂w_i}=\n",
        "\\begin{cases}\n",
        "-tx_i \\ \\ if \\ \\ 1-ts(x)>0\\\\\n",
        "0 \\ \\  \\ \\ \\  \\ \\  otherwise\\\\\n",
        "\\end{cases}$$"
      ],
      "metadata": {
        "id": "QTiI8xbE0FgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "2Cjx2yZw2vA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_pipeline = make_pipeline(KNNImputer(weights='distance'), PolynomialFeatures(degree=2, include_bias=False),\n",
        "                              RobustScaler(), ADASYN(sampling_strategy='minority',random_state=42),\n",
        "                              SGDClassifier(loss='hinge',penalty='elasticnet',\n",
        "                                            class_weight=None, learning_rate='adaptive',\n",
        "                                            random_state=42))\n",
        "\n",
        "param_distribs = {'knnimputer__n_neighbors': randint(low=10, high=500),\n",
        "                  'adasyn__n_neighbors': randint(low=3, high=100),\n",
        "                  'sgdclassifier__alpha': uniform(loc=0.0001, scale=3),\n",
        "                  'sgdclassifier__l1_ratio': uniform(loc=0.1, scale=0.9),\n",
        "                  'sgdclassifier__eta0': uniform(loc=0.0001, scale=10),\n",
        "\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    full_pipeline,\n",
        "    param_distributions=param_distribs,\n",
        "    n_iter=10,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    random_state=42\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "WacMZnK01dfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = random_search.predict(X_test)\n",
        "log_results('SVM', y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yATNRrM52Oex",
        "outputId": "c5d97798-57f8-43e7-b1d2-dcf42dc09ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix for SVM:\n",
            " [[2727  759]\n",
            " [ 212  302]]\n",
            "Precision Score: 0.28\n",
            "Recall Score: 0.59\n",
            "F1 Score: 0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(random_search, 'svm.pki')"
      ],
      "metadata": {
        "id": "A1XY-TJeiPLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernelized SVM"
      ],
      "metadata": {
        "id": "QhbCYEWUoaAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "4RYF4W9poxs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The duality principle, states that a constrained optimization problem can be\n",
        "expressed with a different form known as the dual problem. If the primal problem is a minimization problem then the dual problem is a maximization problem. Finding the values that maximize the dual problem can provide a lower bound for the solution of the primal problem. Although, there are some conditions (Karush-Kuhn-Tucker conditions) that when met ensure that the solution of the two problems is the same. This is true for the SVM objective function.\n",
        "\n",
        "Having a general constrained maximization problem as the one below:\n",
        "$$maximize_{x,y} f(x,y) $$\n",
        "$$ \\text{subject to } g(x, y) =  c $$\n",
        "\n",
        "we need to find the point on the $g(x,y)=c$ that maximizes $f(x, y)$. On that point the gradients of f and g will be parallel but the magnitude can be different. Thus, we cannot simply calculate the derivate of f to solve the optimization problem and we will need to define a new function (the Lagrangian) that includes both the optimization function f and the constraint function g .Here we introduce also the Lagrange Multiplier a to address the fact that magnitude of the functions can be different:\n",
        "$$L(x, y, a)= f(x,y) - a(g(x,y) - c )$$\n",
        "\n",
        "Now we can calculate the partial derivatives of L with repsect to x, y and a and search where they are equal to zero\n",
        "\n",
        "$$ \\frac{∂L}{∂a} = \\frac{∂}{∂a} (f(x,y) - a(g(x,y) - c )) = c - g(x, y)$$\n",
        "\n",
        "Thus we can say that $c - g(x,y) = 0 (1)$\n",
        "\n",
        "$$ \\frac{∂L}{∂x} = \\frac{∂}{∂x} (f(x,y) - a(g(x,y) - c )) = \\frac{∂}{∂x}f(x,y) - a\\frac{∂}{∂x}g(x,y)$$\n",
        "$$ \\frac{∂}{∂x}f(x,y) = a\\frac{∂}{∂x}g(x,y) (2)$$\n",
        "\n",
        "$$ \\frac{∂L}{∂y} = \\frac{∂}{∂y} (f(x,y) - a(g(x,y) - c )) =  \\frac{∂}{∂y}f(x,y) - a\\frac{∂}{∂y}g(x,y) $$\n",
        "\n",
        "$$ \\frac{∂}{∂y}f(x,y) = a\\frac{∂}{∂y}g(x,y)(3)$$\n",
        "\n",
        "Now we have the equations (1)(2)(3) and three unknowns so we can find the point for which\n",
        "\n",
        "$$ \\frac{∂L}{∂x, y, a} = 0$$\n",
        "\n",
        "\n",
        "In the example above we had an equality constraint for the optimization, but both the hard margin and soft margin classfications as defined in the \"SGD Classifier - SVM\" section have an inequality constraint. We can add an inequality constraint to the example above so we have:\n",
        "\n",
        "$$maximize_{x} f(x)$$\n",
        "$$\\text{subject to } g(x) ≤ 0 $$\n",
        "$$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ h(x) = 0$$\n",
        "\n",
        "Now the Lagrangian function can be written as:\n",
        "$$L(x, a, b) = f(x) + ag(x) +bh(x)$$\n",
        "\n",
        "In this case we cannot just find the gradients that are equal to zero but also the KKT conditions must be met for the solution of both problems to be the same. So in the addition of Lagrangian function derivatives to be zero we also need:\n",
        "$$ ag(x)  = 0$$\n",
        "$$ g(x) \\le 0$$\n",
        "$$ a \\ge 0$$\n",
        "\n",
        "\n",
        "The harg margin objective function is :\n",
        "$$minimize_{w} \\frac{1}{2}w^Tw$$\n",
        "$$\\text{subject to } t^{(i)}(w^Tx^{(i)} + b) ≥ 1$$\n",
        "Due to second KKT condition ($g(x) \\le 0$) we can write the constraint as:\n",
        "$$1 -  t^{(i)}(w^Tx^{(i)} + b) ≤ 0$$\n",
        "\n",
        "And we can defind the Lagrangian (the Lagrangian now has a sum because the constraint is applied for each instance with index i, thus we have multiple constraints)\n",
        "\n",
        "$$minimize_{w} \\frac{1}{2}w^Tw + \\sum_{i=0}^{n}a_i(1-t_i(w^Tx_i + b)) = \\frac{1}{2}w^Tw - \\sum_{i=0}^{n}a_i(t_i(w^Tx_i + b)-1)$$\n",
        "\n",
        "Now based on the other two inequality conditions we have:\n",
        "$$a_i(1-t_i(w^Tx_i +b)) = 0 (4) $$\n",
        "$$a_i ≥ 0 $$\n",
        "\n",
        "And also the partial derivatives with respect to w and b must be equal to zero:\n",
        "\n",
        "$$\\frac{∂L}{∂w} = \\frac{∂}{∂w} (\\frac{1}{2}w^Tw - \\sum_{i=0}^{n}a_i(t_i(w^Tx_i + b)-1)) = \\frac{∂}{∂w} \\frac{1}{2}||w||^2 - ∑_{i=0}^n \\frac{∂}{∂w}(a_i(t_i(w^Tx_i + b)-1)) $$\n",
        "\n",
        "$$ w - \\sum_{i=0}^na_it_ix_i=0$$\n",
        "$$ w = \\sum_{i=0}^na_it_ix_i (5) $$\n",
        "\n",
        "\n",
        "$$ \\frac{∂L}{∂b} = \\frac{∂}{∂b} \\frac{1}{2}||w||^2 - ∑_{i=0}^n \\frac{∂}{∂b}(a_i(t_i(w^Tx_i + b)-1)) = $$\n",
        "\n",
        "$$∑_{i=0}^na_it_i = 0$$\n",
        "\n",
        "We can use those on the Lagrangian:\n",
        "$$\\frac{1}{2}w^Tw - \\sum_{i=0}^{n}a_i(t_i(w^Tx_i + b)-1)= \\frac{1}{2}w^Tw -\\sum_{i=0}^n a_it_iw^Tx_i - \\sum_{i=0}^na_it_ib + \\sum_{i=0}^na_i = \\frac{1}{2}w^Tw -w^T\\sum_{i=0}^n a_it_ix_i - b\\sum_{i=0}^na_it_i + \\sum_{i=0}^na_i =  \\frac{1}{2}w^Tw -w^Tw- b ⋅ 0 + \\sum_{i=0}^na_i = -\\frac{1}{2}w^Tw + \\sum_{i=0}^na_i = \\sum_{i=0}^na_i  - \\frac{1}{2}(\\sum_{i=0}^na_it_ix_i \\sum_{i=0}^na_it_ix_i) =\\sum_{i=0}^na_i  - \\frac{1}{2}\\sum_{i=0}^n\\sum_{j=0}^na_ia_jt_it_jx_i^Tx_j $$\n",
        "\n",
        "So the dual problem can be written as:\n",
        "$$ maximize_a \\sum_{i=0}^na_i  - \\frac{1}{2}\\sum_{i=0}^n\\sum_{j=0}^na_ia_jt_it_jx_i^Tx_j (6)$$\n",
        "$$\\text{subject to } a_i \\ge 0 , \\sum_{i=0}^n a_it_i = 0$$\n",
        "After we calculated the optimal value of $a$ we can calculate the weight and bias. For the weights we can use eq.5, and we can solve for the bias in eq.4 and then average the result:\n",
        "$$ a_i(1-t_i(w^Tx_i +b)) = 0 $$\n",
        "$$ b_i = \\frac{1}{t_i} - w^Tx_i$$\n",
        "Now since t is either -1 or 1 it is true that $t_i = \\frac{1}{t_i}$\n",
        "$$b_i = t_i - w^Tx_i $$\n",
        "$$b = \\frac{1}{n_s}\\sum_{i=0}^nt_i - w^Tx_i (7)$$\n",
        "\n",
        "Now that we defined the dual problem we can specify why we need it. In the previous sections we saw that one technique to get better perfomance is the addition of new features via the PolynomialFeatures class. That increases the dimensionality of the dataset providing us with more information and giving a better change to find a linear seperation between the classes. The dual problem makes it possible to get the same result without actually needing to add the extra features to the dataset. This is possible via a mathematical trick, which known as the kernel method.\n",
        "\n",
        "First lets define a transformation that we want to apply in our data. The following is a second-degree polynomial transformation, that transforms a vector from 2D to 3D:\n",
        "\n",
        "$$φ\n",
        "\\begin{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{pmatrix} =\n",
        "\\begin{pmatrix}\n",
        "x_1^2 \\\\\n",
        "\\sqrt{2}x_1x_2 \\\\\n",
        "x_2^2 \\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "\n",
        "Now if we apply that transformation for all of the instances we need to chabge eq.6 to:\n",
        "\n",
        "$$ maximize_a \\sum_{i=0}^na_i  - \\frac{1}{2}\\sum_{i=0}^n\\sum_{j=0}^na_ia_jt_it_jφ(x_i)^Tφ(x_j) (8)$$\n",
        "$$\\text{subject to } a_i \\ge 0 , \\sum_{i=0}^n a_it_i = 0$$\n",
        "\n",
        "So now the dot product of the transformations appears in the dual problem. Let's see what happens if we calculate the dot product of the of the transformations for two vector a and b:\n",
        "\n",
        "$$φ(a)^Tφ(b) =\n",
        "\\begin{pmatrix}\n",
        "a_1^2 \\\\\n",
        "\\sqrt{2}a_1a_2 \\\\\n",
        "a_2^2 \\\\\n",
        "\\end{pmatrix}^T\n",
        "\\begin{pmatrix}\n",
        "b_1^2 \\\\\n",
        "\\sqrt{2}b_1b_2 \\\\\n",
        "b_2^2 \\\\\n",
        "\\end{pmatrix} =\n",
        "$$\n",
        "$$a_1^2b_1^2 + 2a_1b_1a_2b_2 + a_2^2b_2^2 = (a_1b_1 + a_2b_2)^2  =\n",
        "\\begin{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "a_1 \\\\\n",
        "a_2 \\\\\n",
        "\\end{pmatrix}^T\n",
        "\\begin{pmatrix}\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{pmatrix}^2 =\n",
        "(a^Tb)^2\n",
        "$$\n",
        "\n",
        "The dot product of the transformations is equal to the squared dot product of the original vectors. That means that we do not need to apply the transformation at all and eq.8 is the same as:\n",
        "$$ maximize_a \\sum_{i=0}^na_i  - \\frac{1}{2}\\sum_{i=0}^n\\sum_{j=0}^na_ia_jt_it_j(x_i^Tx_j)^2 $$\n",
        "$$\\text{subject to } a_i \\ge 0 , \\sum_{i=0}^n a_it_i = 0$$\n",
        "\n",
        "Kernel is a function that calculate the dot product of the transformations of two vectors without actually needing to calculate the transformation. In our case the polynomial transformation is just replaced with the squarred dot product of the vectors, but there are more kernels that lets us replace other transformations as the Gaussian RBF and the Sigmoid.\n",
        "\n",
        "Now the only thing remaing is how we can find the weights and the bias when we use the kernel method. For the w we cannot directly use eq.5 anymore because if we do we would have to calculate the transformation. Although we can replace the weight matrix with eq.5 in the decision function and we can see that the terms can be simplified:\n",
        "$$ f(φ(x_n)) = w^Tφ(x_n) + b = (\\sum_{i=0}^m a_it_iφ(x_i))^Tφ(x_n) +  b = \\sum_{i=0}^ma_it_i(φ(x_i)^Tφ(x_n)) + b = \\sum_{i=0}^m a_it_iK(x_i, x_n) + b$$\n",
        "\n",
        "With $K(x_i, x_n)$ being the kernel function for the transformation we use.\n",
        "\n",
        "Now for the bias we can in eq.7 plug eq.5:\n",
        "$$b = \\frac{1}{n_s} \\sum_{i=0}^m t_i - w^Tφ(x_i)= \\frac{1}{n_s} \\sum_{i=0}^m (t_i - (\\sum_{j=0}^ma_jt_jφ(x_j))φ(x_i)) = \\frac{1}{n_s} \\sum_{i=0}^m (t_i - \\sum_{j=0}^ma_jt_jK(x_i, x_j))$$"
      ],
      "metadata": {
        "id": "xokoIMZwozQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model"
      ],
      "metadata": {
        "id": "GFCDlh-movtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### poly"
      ],
      "metadata": {
        "id": "u2nnh7U01wZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "full_pipeline = make_pipeline(KNNImputer(weights='distance'), ADASYN(sampling_strategy='minority'), RobustScaler(),\n",
        "                              SVC(kernel='poly', degree=2, random_state=42))\n",
        "\n",
        "param_distribs = {\n",
        "    'adasyn__n_neighbors': randint(low=3, high=100),\n",
        "    'knnimputer__n_neighbors': randint(low=5, high=100),\n",
        "    'svc__C': randint(low=1, high=100),\n",
        "    'svc__coef0': randint(low=1, high=100)\n",
        "}\n",
        "\n",
        "random_search =  RandomizedSearchCV(\n",
        "    full_pipeline,\n",
        "    param_distributions=param_distribs,\n",
        "    n_iter=5,\n",
        "    scoring=\"f1\",\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "9HkWzLr1ob_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a289cf-2b9c-4b8a-b3c0-07feb3e03321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adasyn__n_neighbors': 63,\n",
              " 'knnimputer__n_neighbors': 25,\n",
              " 'svc__C': 83,\n",
              " 'svc__coef0': 87}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = random_search.predict(X_test)\n",
        "log_results('SVM_Poly', y_test, y_pred)"
      ],
      "metadata": {
        "id": "3Zp0VCEUokCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccc9139-f77f-4aff-b99a-554ac41463f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix for SVM_Poly:\n",
            " [[2498  988]\n",
            " [ 140  374]]\n",
            "Precision Score: 0.27\n",
            "Recall Score: 0.73\n",
            "F1 Score: 0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(random_search, 'svm_poly.pki')"
      ],
      "metadata": {
        "id": "0nzJtymhl202"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rbf"
      ],
      "metadata": {
        "id": "6AKbDdMM1y37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "full_pipeline = make_pipeline(KNNImputer(weights='distance'), ADASYN(sampling_strategy='minority'), RobustScaler(),\n",
        "                              SVC(kernel='rbf', degree=2, random_state=42))\n",
        "\n",
        "param_distribs = {\n",
        "    'adasyn__n_neighbors': randint(low=3, high=100),\n",
        "    'knnimputer__n_neighbors': randint(low=5, high=100),\n",
        "    'svc__C': randint(low=1, high=100),\n",
        "    'svc__coef0': randint(low=1, high=100)\n",
        "}\n",
        "\n",
        "random_search =  RandomizedSearchCV(\n",
        "    full_pipeline,\n",
        "    param_distributions=param_distribs,\n",
        "    n_iter=10,\n",
        "    scoring=\"f1\",\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "QhfSYUIQLTgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472e98f3-28ab-4d66-ad99-4d5f9de48ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adasyn__n_neighbors': 77,\n",
              " 'knnimputer__n_neighbors': 79,\n",
              " 'svc__C': 88,\n",
              " 'svc__coef0': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = random_search.predict(X_test)\n",
        "log_results('SVM_RBF', y_test, y_pred)"
      ],
      "metadata": {
        "id": "JrrTdni9LZXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b7dbbc-8737-44b9-b49b-c9eea528a12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix for SVM_RBF:\n",
            " [[2525  961]\n",
            " [ 154  360]]\n",
            "Precision Score: 0.27\n",
            "Recall Score: 0.70\n",
            "F1 Score: 0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(random_search, 'svm_rbf.pki')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yt0fYd4v9_h",
        "outputId": "aaa6288f-738e-4ff0-90f2-cc8d2bde892e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_rbf.pki']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb37TFqTTRZi"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "LZM8zpDgQ_ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees work by searching the feature and threshold that best splits the data in each node. Each node asks a True/False question for one feature of the instances (is the feature f of instance x greater than a threshold t) and then seperate that data into two different nodes based on the answer. Thus each instance traverses the tree answering those quesions on each node and then follows the path that the tree has created for each answer until it reaches a leaf node. A leaf node does not have any children and based on the leaf each instance reached a prediction can be made. The goal is to create a tree in such a way that can lead all the instances of a class to the same leaf.\n",
        "\n",
        "Each node tries to find the best feature-threshold combination that it will produce more pure nodes. A node is pure if all the training instances that are associated with it belong to the same class. The first way to measure the purity of a node is Gini Impurity:\n",
        "$$ G_i = 1 - \\sum_{k=1}^n p_{i,k}^2$$\n",
        "\n",
        "$p_{i,k}$ is the ratio of class k instances among the training instances in the node. Thus a node is deemed to be total pure if Gini impurity is equal to zero meaning that all instances belong to the same class.\n",
        "\n",
        "Another common way to measure the purity of a node is the Entropy measure:\n",
        "$$H_i = - \\sum_{k=1}^n p_{i,k}log_2(p_{i, k}) $$\n",
        "\n",
        "Most of the times both measures create a similar tree.\n",
        "\n",
        "Based on these perfomances the model now has to pick the best feature/threshold on each node that increases the total purity of the children nodes. One common algorithm (used by Scikit-Learn) is the Classification and Regression Tree (CART) that uses the following cost function:\n",
        "$$J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right} $$\n",
        "\n",
        "Which is the sum impurity of the two new nodes weighted by the number of instances associated with that node.\n",
        "\n",
        "This is greedy algorithm meaning that the algorithm tries to find the optimal choice at each step but does not look if that choice will later lead to the optimal path or not. Finding an optimal tree is for now a NP-complete problem.\n",
        "\n",
        "\n",
        "One of the most known methods in machine learning is the combination of a lot of decision trees known as Random Forests. To create differences between the decision trees, random forests train each decision tree on a subset of the features. This  means that now when splitting each node the algorithm does not consider all the features of the dataset but only random subset of them, most commonly $\\sqrt{n}$ features with n being the total number of features."
      ],
      "metadata": {
        "id": "LZWWTtcZRBPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "80mpGAkCQ-Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "full_pipeline = make_pipeline(KNNImputer(weights='distance'), PolynomialFeatures(degree=2),\n",
        "                              ADASYN(), RobustScaler(),\n",
        "                              RandomForestClassifier(max_depth=12, random_state=42))\n",
        "\n",
        "\n",
        "param_distibs = {\n",
        "    'knnimputer__n_neighbors': randint(low=5, high=100),\n",
        "    'adasyn__sampling_strategy': uniform(loc=0.1, scale=0.9),\n",
        "    'adasyn__n_neighbors': randint(low=3, high=50),\n",
        "    'randomforestclassifier__class_weight': [None, 'balanced'],\n",
        "    'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
        "    'randomforestclassifier__n_estimators': randint(low=2, high=20),\n",
        "\n",
        "}\n",
        "\"\"\"\n",
        "'randomforestclassifier__max_depth': randint(low=8, high=20),\n",
        "'randomforestclassifier__min_samples_split':randint(low=3, high=7),\n",
        "'randomforestclassifier__min_samples_leaf': randint(low=2, high=6),\n",
        "\"\"\"\n",
        "\n",
        "rand_search =  RandomizedSearchCV(\n",
        "    full_pipeline,\n",
        "    param_distributions=param_distibs,\n",
        "    n_iter=10,\n",
        "    scoring=\"f1\",\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "rand_search.fit(X_train, y_train)\n",
        "rand_search.best_params_"
      ],
      "metadata": {
        "id": "NSPOw27c2Moy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a3be3e-0901-47d3-9e60-bfdf2bf407b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adasyn__n_neighbors': 14,\n",
              " 'adasyn__sampling_strategy': 0.6504878444394528,\n",
              " 'knnimputer__n_neighbors': 93,\n",
              " 'randomforestclassifier__class_weight': None,\n",
              " 'randomforestclassifier__criterion': 'gini',\n",
              " 'randomforestclassifier__n_estimators': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWWsGZMZMaUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb20b13-3f91-493c-aff6-496acffb1992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix for RF:\n",
            " [[3355  131]\n",
            " [ 288  226]]\n",
            "Precision Score: 0.63\n",
            "Recall Score: 0.44\n",
            "F1 Score: 0.52\n"
          ]
        }
      ],
      "source": [
        "y_pred = rand_search.predict(X_test)\n",
        "log_results('RF', y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(random_search, 'rf.pki')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRLL-IAWmOCT",
        "outputId": "1e54019e-fba8-4807-c628-d4af0d6ce970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf.pki']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k33jFuVj7Nax"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "Wy2mIjfeuoQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the simpliest and most used parts of neural networks are the perceptrons. A perceptron has one weight for each input that receives, so if the perceptron is placed on a part of the network that receives 3 inputs then the perceptron is going to have 3 weights. The role of the perceptron is to scale the incoming inputs by the assoicated weights and then sum the results. Then an activation function can be applied to scale the result of the perceptron to an appropiate range depending on the task. Calculating the result of a perceptron is as simply as:\n",
        "$$h_w(x) = φ(w^tx + b) $$\n",
        "With φ() being the activation function\n",
        "\n",
        "Using just one perceptron in most cases does not result in great results as it just performs a linear function of its inputs. We can use multiple perceptrons and pass our input data to all of them and then combine the results thus creating a layer of perceptrons. But we can go a step further and use multiple layers of perceptrons, each one passing their results to the next one until it reaches the final layer and it ouputs the results. With that we can create a network of perceptrons. That structure has been shown that it can capture very complicated data relations.\n",
        "\n",
        "The use of activation function depends on the part of the model and the task at hand. For the hidden layers (the ones between the input and the output layer) the role of the activation function is to scale the results in a way that boosts and stabilizes the training of the model. In those cases one of the most used activation functions is the Rectified Linear Unit (ReLU):\n",
        "$$f(x) = max(0, x)$$\n",
        "\n",
        "In some cases a percepton may stuck on a parameter space that gives negative results and it is hard to get out because ReLU zeros out all negative results therefore there is not gradient for the parameters to update. Still ReLU is one of the most popular activation functions because it has a great perfomance, does not need parameterization, and that problem can be reduced with some techniques such as the initilization of the parameters.\n",
        "\n",
        "For the output layer the activation function deppends on what the model needs to predict. For example in our case, which is a binary classification, we just need the model to predict the probability of an instance belong to the positive instance, $p$ , and we know that probability for the negative instance will be $1-p$. In those cases, the most popular activation functioin is the sigmoid function:\n",
        "\n",
        "$$σ(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "The output of this function is in the range of [0, 1] and represents the probability $p$.\n",
        "\n",
        "We also need to define a loss function for the model and for binary classification a common one is the binary crossentropy (or log loss):\n",
        "\n",
        "$$J = -\\frac{1}{m}\\sum_{i=0}^my_ilog(p) + (1-y_i)log(1-p)$$\n",
        "\n",
        "To train the model we need to update the weight parameters in a way that it minimizes the loss function. Once again we can use gradient descent for that as described in the \"SGDClassifier-LogisticRegression\" section.\n",
        "\n",
        "To use gradient descent we need to calculate the partial derivative of the loss function with respect to each parameter of the model which is going to give us the amount that we need to adjust each parameter for each training step. The difference here is that using a network structure results in parameters which are in different levels. We can easily calculate the partial derivates with respect to the parameters of the last level but we must find a way to do the same for the previous layers also.\n",
        "\n",
        "This is where the backpropagation algorithm comes in. The backpropagation algorithm can calculate the gradient for each parameter of the model in just two passes, one forward and one backward. In the forward pass the network accepts the input data and makes the calculations for each layer passing the results in the next layer until it reaches the final layer. During this process the output of each neuron is saved because it is needed for the gradient calculation. Based on the output of the final layer the loss function is calculated to find out how well the model is performing. Now the backward pass is starting with the goal of medidating the loss from the last layer all the way to the first layer. As we mentioned for the last layer we can simply calculate the partial derivatives as it is directly linked with the loss functions. Once we calculate the gradients for the last layer we can move to the previous one and this time we need to use the chain rule to take into consideration the gradients that was just calculated. The process continues until it reaches the input layer where at this point the gradient of each parameter would been calculated. To update the parameters we just subtract the equivalent gradient scaled by the learning rate as it is described in the \"SGDClassifier-LogisticRegression\" section."
      ],
      "metadata": {
        "id": "s0TCom9iuprZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "KQ9vvDgGumS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z51rCSk29nqn"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(KNNImputer(weights='distance', n_neighbors=50), PolynomialFeatures(degree=2), RobustScaler())\n",
        "X_train = pipeline.fit_transform(X_train, y_train)\n",
        "y_train = tf.cast(y_train, dtype=tf.float32)\n",
        "adasyn = ADASYN(sampling_strategy='minority', n_neighbors=25, random_state=42)\n",
        "X_train_res, y_train_res = adasyn.fit_resample(X_train, y_train)\n",
        "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(X_train_res, y_train_res, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XFFxEig0L8dy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_res, y_train_res))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_res, y_val_res))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1000, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(500, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(25, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_f1', patience = 5, mode='max', restore_best_weights = True)\n",
        "callbacks = [early_stopping_cb]\n",
        "\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0001,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=False,\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[tf.keras.metrics.F1Score(name='f1')])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=70,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "YRLKStIH21VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pipeline.transform(X_test)\n",
        "y_pred = [1 if p > 0.5 else 0 for p in model.predict(X_test)]\n",
        "log_results('DNN', y_test, y_pred)"
      ],
      "metadata": {
        "id": "gDeFioNsLURK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('dnn.h5')"
      ],
      "metadata": {
        "id": "5JY_h5lc67JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Results"
      ],
      "metadata": {
        "id": "BerSeJHeKtt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"Name\", \"Recall\", \"Precision\", \"F1\"]\n",
        "table = tabulate(results, headers, tablefmt=\"grid\", floatfmt=\".2f\")\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs3EpzV3KwmD",
        "outputId": "ee6ec09d-e4c7-4d03-a02e-79cca8d0d7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-------------+------+\n",
            "| Name     |   Recall |   Precision |   F1 |\n",
            "+==========+==========+=============+======+\n",
            "| LogLoss  |     0.28 |        0.59 | 0.38 |\n",
            "+----------+----------+-------------+------+\n",
            "| SVM      |     0.28 |        0.59 | 0.38 |\n",
            "+----------+----------+-------------+------+\n",
            "| SVM_Poly |     0.27 |        0.73 | 0.40 |\n",
            "+----------+----------+-------------+------+\n",
            "| SVM_RBF  |     0.27 |        0.70 | 0.39 |\n",
            "+----------+----------+-------------+------+\n",
            "| RF       |     0.63 |        0.44 | 0.52 |\n",
            "+----------+----------+-------------+------+\n",
            "| DNN      |     0.55 |        0.51 | 0.53 |\n",
            "+----------+----------+-------------+------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "j1E6vpILbQjv",
        "UTLGU3-THZTj",
        "R3xQf-pyvKoC",
        "y9ZwB3XWGzcK",
        "lfCW0WxF4E_-",
        "n7IG-760FETH",
        "5uCv1sfpP4-g",
        "cyoPXjB3ANj8",
        "Tb37TFqTTRZi",
        "k33jFuVj7Nax"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}